# DSP Project Report (Run run_20251221_003516)

Run directory: `outputs/results/run_20251221_003516`

This report is a comprehensive, end-to-end record of all experiments executed for the run, including settings, outputs, and observed behavior. It also provides pointers to every raw file for traceability.

## 1. Overview
- Objective: build a full ESC-50 audio pipeline with custom DSP features, MFCC retrieval, CNN classification, transfer learning baselines (PANNs/AST/CLAP), CLAP zero-shot, and LLM (Gemini) baselines.
- Run ID: `run_20251221_003516`.
- Primary outputs are in `outputs/results/run_20251221_003516/` (see links at the end).

## 2. Dataset and Protocol
- Dataset: ESC-50 (50 classes, 2000 clips, ~5 seconds each).
- Data root: `data/ESC-50-master`.
- Fold protocol: folds 1-4 for training and retrieval database; fold 5 for test and retrieval queries.
- Sample rate: 44100 Hz (resampled at load time).

## 3. Feature Extraction (Custom DSP)
All DSP is custom-implemented; no numpy.fft or scipy dct is used in the core pipeline.

### 3.1 FFT / STFT
- FFT: radix-2 Cooley-Tukey, bit reversal, power-of-two padding in `src/dsp/fft.py`.
- STFT: custom framing + Hann window + custom RFFT per frame in `src/dsp/stft.py`.
- Window: `hann` by default.

### 3.2 Log-mel and MFCC
- Pre-emphasis: 0.97.
- n_mels: 40.
- n_mfcc: 13.
- log-mel: power spectrum -> mel filterbank -> log.
- MFCC: DCT-II on log-mel.

### 3.3 Caching and Precompute
- Feature cache root: `outputs/results/run_20251221_003516/features/`.
- Cached by feature type and parameter hash, organized by fold/filename.
- Precompute logs (4x4 grid) in `outputs/results/run_20251221_003516/logs/precompute_*`.

## 4. DSP Validation vs librosa
- File: `outputs/results/run_20251221_003516/validation/librosa_compare.json`.
- Example (file `5-103415-A-2.wav`):
  - STFT complex relative error: 2.54e-08
  - STFT magnitude relative error: 4.77e-08
  - log-mel relative error: 0.0619
  - MFCC relative error: 0.0252
- Interpretation: STFT aligns extremely well; log-mel and MFCC have small relative differences due to filterbank/DCT implementation details.

## 5. Task 1: MFCC Retrieval (No ML)

### 5.1 Method
- MFCC embedding per clip = concat(mean(MFCC), std(MFCC)) across time.
- Similarity = cosine similarity; evaluation = top-k hit rate.
- k_list = [10, 20].

### 5.2 Hyperparameter Grid
- frame_length in {512, 1024, 2048, 4096}
- hop_length in {256, 512, 1024, 2048}
- n_mels = 40, n_mfcc = 13, sample_rate = 44100
- Results in `outputs/results/run_20251221_003516/retrieval/retrieval_mfcc.csv`

### 5.3 Full Results (Top-10)
| frame\hop | 256 | 512 | 1024 | 2048 |
| --- | --- | --- | --- | --- |
| 512 | 0.6500 | 0.6475 | 0.6450 | 0.6525 |
| 1024 | 0.6575 | 0.6525 | 0.6525 | 0.6400 |
| 2048 | 0.6775 | 0.6775 | 0.6625 | 0.6600 |
| 4096 | 0.6575 | 0.6575 | 0.6625 | 0.6600 |

### 5.4 Full Results (Top-20)
| frame\hop | 256 | 512 | 1024 | 2048 |
| --- | --- | --- | --- | --- |
| 512 | 0.7775 | 0.7800 | 0.7775 | 0.7725 |
| 1024 | 0.7925 | 0.7900 | 0.7900 | 0.7875 |
| 2048 | 0.7950 | 0.7950 | 0.7950 | 0.7925 |
| 4096 | 0.7800 | 0.7775 | 0.7775 | 0.7750 |

### 5.5 Key Observations
- Best top-10: frame 2048 with hop 256 or 512 (0.6775).
- Best top-20: frame 2048 with hop 256/512/1024 (0.7950).
- Large window (4096) does not improve retrieval; mild drop suggests over-smoothing or too few frames.

### 5.6 Artifacts
- Table: `outputs/results/run_20251221_003516/retrieval/retrieval_mfcc.csv`
- Log: `outputs/results/run_20251221_003516/logs/retrieval_mfcc_grid.log`
- Heatmaps: `outputs/results/run_20251221_003516/plots/retrieval_mfcc_top10_heatmap.png`, `outputs/results/run_20251221_003516/plots/retrieval_mfcc_top20_heatmap.png`

## 6. Task 2: CNN Classification (ResNet-like, from scratch)

### 6.1 Model
- Architecture: `src/models/resnet.py` (ResNet-style)
  - Stem: Conv3x3 + BN + ReLU
  - 4 stages, channels (16, 32, 64, 128), blocks (2,2,2,2)
  - AdaptiveAvgPool2d(1,1) + Linear classifier
- Input: log-mel spectrogram (1 channel)

### 6.2 Training Settings
- Optimizer: Adam, lr 1e-3.
- Epochs: 50.
- Batch size: 32.
- Input hyperparameters (best): frame_length 2048, hop_length 1024, n_mels 40.
- Training history: `outputs/results/run_20251221_003516/history/train_cnn.csv`
- Model: `outputs/results/run_20251221_003516/models/cnn.pt`

### 6.3 Best Test Accuracy
- Best test acc: **0.7500** at epoch 48.
- Note: epochs 49-50 show sharp degradation (likely overfitting / instability).

### 6.4 Artifacts
- Predictions: `outputs/results/run_20251221_003516/predictions/cnn_fold5.csv`
- Loss curve: `outputs/results/run_20251221_003516/plots/cnn_history.png`

## 7. Classification Hyperparameter Grid (4x4)

### 7.1 Grid Settings
- frame_length: [512, 1024, 2048, 4096]
- hop_length: [256, 512, 1024, 2048]
- epochs: 50, batch_size: 32, n_mels: 40

### 7.2 Full Results (Best Acc)
| frame\hop | 256 | 512 | 1024 | 2048 |
| --- | --- | --- | --- | --- |
| 512 | 0.6725 | 0.6975 | 0.7025 | 0.6450 |
| 1024 | 0.7025 | 0.6950 | 0.7225 | 0.6700 |
| 2048 | 0.6925 | 0.7150 | 0.7500 | 0.7200 |
| 4096 | 0.6650 | 0.7000 | 0.7100 | 0.7000 |

### 7.3 Key Observations
- Best overall: frame 2048, hop 1024 (0.7500).
- 2:1 ratio (frame/hop) consistently strong (1024/512, 2048/1024).
- Very large frames (4096) help less than 2048.

### 7.4 Artifacts
- Aggregate: `outputs/results/run_20251221_003516/history/classification_grid.csv`
- Per-combo histories: `outputs/results/run_20251221_003516/history/classification_grid/frame*_hop*.csv`
- Heatmap: `outputs/results/run_20251221_003516/plots/classification_grid_heatmap.png`

## 8. Retrieval with ML Embeddings

### 8.1 Method
- For CNN, PANNs, AST, CLAP: compute embeddings for db/query, then run cosine top-k retrieval.
- Results are in logs.

### 8.2 Results
- CNN (2048/1024): top10 0.8550, top20 0.8850
  - Log: `outputs/results/run_20251221_003516/logs/retrieval_ml_cnn.log`
- PANNs: top10 0.9775, top20 0.9800
  - Log: `outputs/results/run_20251221_003516/logs/retrieval_ml_panns.log`
- AST: top10 0.9875, top20 0.9925
  - Log: `outputs/results/run_20251221_003516/logs/retrieval_ml_ast.log`
- CLAP: top10 0.9950, top20 1.0000
  - Log: `outputs/results/run_20251221_003516/logs/retrieval_ml_clap.log`

### 8.3 Observations
- Pretrained embeddings (AST/CLAP) dominate MFCC retrieval.
- CLAP achieves perfect top-20; retrieval is near-saturated with strong embeddings.

## 9. Zero-shot CLAP
- Predictions: `outputs/results/run_20251221_003516/predictions/clap_zeroshot.csv`
- Accuracy: **0.9150** (from `outputs/results/run_20251221_003516/errors/clap_zeroshot_errors.csv`)
- Log: `outputs/results/run_20251221_003516/logs/clap_zeroshot.log`
- Error details in `outputs/results/run_20251221_003516/errors/error_analysis.md`

## 10. Transfer Learning (Linear Head on Embeddings)

### 10.1 Results
- CLAP transfer: acc **0.9725**
  - `outputs/results/run_20251221_003516/history/clap_transfer.csv`
  - `outputs/results/run_20251221_003516/models/clap_transfer.pt`
- AST transfer: acc **0.9500**
  - `outputs/results/run_20251221_003516/history/ast_transfer.csv`
  - `outputs/results/run_20251221_003516/models/ast_transfer.pt`
- PANNs transfer: acc **0.9050**
  - `outputs/results/run_20251221_003516/history/panns_transfer.csv`
  - `outputs/results/run_20251221_003516/models/panns_transfer.pt`

### 10.2 Observations
- CLAP transfer is strongest overall, AST slightly behind.
- PANNs is solid but lower than CLAP/AST.

## 11. LLM Baseline (Gemini)

### 11.1 Predictions
- Base prompt: `outputs/results/run_20251221_003516/predictions/llm_predictions.csv`
- Guided prompt: `outputs/results/run_20251221_003516/predictions/llm_predictions_guided.csv`

### 11.2 Accuracy
- Base prompt accuracy: **0.7800**
- Guided prompt accuracy: **0.7525**
- Prompt comparison: `outputs/results/run_20251221_003516/errors/prompt_comparison.md`

### 11.3 Observations
- Guided prompt did not improve accuracy; base prompt remains better.
- Common failures include confusions among acoustically similar classes.

## 12. Error Analysis and Audio Review
- Consolidated analysis: `outputs/results/run_20251221_003516/errors/error_analysis.md`
- Error CSVs for each model: `outputs/results/run_20251221_003516/errors/*_errors.csv`
- Error audio copies (renamed with gt/pred): `outputs/results/run_20251221_003516/errors/audio/`

## 13. Plots
- CNN training curve: `outputs/results/run_20251221_003516/plots/cnn_history.png`
- Classification grid heatmap: `outputs/results/run_20251221_003516/plots/classification_grid_heatmap.png`
- Retrieval MFCC heatmaps: `outputs/results/run_20251221_003516/plots/retrieval_mfcc_top10_heatmap.png`, `outputs/results/run_20251221_003516/plots/retrieval_mfcc_top20_heatmap.png`

## 14. Summary of Best Results (This Run)
- MFCC retrieval: top10 0.6775 (frame 2048 hop 256/512), top20 0.7950 (frame 2048 hop 256/512/1024)
- CNN classification: **0.7500** (frame 2048, hop 1024)
- Transfer models:
  - CLAP transfer 0.9725
  - AST transfer 0.9500
  - PANNs transfer 0.9050
- Zero-shot CLAP: 0.9150
- LLM baseline: base 0.7800, guided 0.7525
- ML retrieval (top10/top20): CNN 0.855/0.885, PANNs 0.9775/0.9800, AST 0.9875/0.9925, CLAP 0.995/1.000

## 15. Raw Output Links (Index)
- Summary: `outputs/results/run_20251221_003516/summary.json`
- DSP validation: `outputs/results/run_20251221_003516/validation/librosa_compare.json`
- Logs: `outputs/results/run_20251221_003516/logs/`
- Features: `outputs/results/run_20251221_003516/features/`
- Predictions: `outputs/results/run_20251221_003516/predictions/`
- Models: `outputs/results/run_20251221_003516/models/`
- Error analysis: `outputs/results/run_20251221_003516/errors/`
- History: `outputs/results/run_20251221_003516/history/`
- Plots: `outputs/results/run_20251221_003516/plots/`
